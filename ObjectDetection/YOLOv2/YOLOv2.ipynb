{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO9000: Better, Faster, Stronger\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "- novel and drawn from prior works method により state-of-the-art かつ高速\n",
    "- joint training により YOLO9000 では detection data を持たない class についても予測できるようになった\n",
    "- we validate our model on ImageNet detection task\n",
    "    - 200 class 中 44 class にしか detection data がない ImageNet detection validation set に対して 19.7mAP\n",
    "    - detection data がない 156 class に対しても 16.0mAP\n",
    "- real-time で 9,000 classes 以上を検出可能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Introduction\n",
    "- detection は classification に比べ dataset に制約がある\n",
    "    - most common classification datasets: 数十万クラス，何百万枚の画像\n",
    "    - most common detection datasets: 数千クラス，数千から数十万枚の画像\n",
    "- detection も classification の scale にしたいがラベルづけが大変，当分無理そう\n",
    "- 既存の large amount of classification data を使って detection system の scope を expand する手法を開発\n",
    "    - classification の hierarchical (階層的な) 視点を用い，複数のデータセットを統合することに成功した\n",
    "- detection data からも classification data からも学習を可能にする joint training algorithm を提案\n",
    "    - classification images を使って detection の性能を向上 (leverage)\n",
    "- 本論文の構成は以下：\n",
    "    1. YOLO を improve して YOLOv2 に\n",
    "    2. dataset combination method と joint training algorithm の導入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Better\n",
    "###  機械学習の評価方法\n",
    "|$~~~~~~~~~~~$|$y=1$|$y=0$|\n",
    "|----------------:|:-:|:-:|\n",
    "|$\\displaystyle{\\hat{y}}=1$|True Positive (TP)|False Positive (FP)|\n",
    "|$\\displaystyle{\\hat{y}}=0$|False Negative (FN)|True Negative (TN)|\n",
    "\n",
    "\n",
    "- Accuracy: 正解率，$\\frac{TP+TF}{TP+FP+FN+TN}$\n",
    "    - 全体のうちどれだけあってるか\n",
    "- Precision: 適合率，$\\frac{TP}{TP+FP}$\n",
    "    - positive と予測したもののうちどれだけあってるか (これが低いとがむしゃらに true って言ってることになる)\n",
    "- Recall: 再現率，$\\frac{TP}{TP+FN}$\n",
    "    - 正しいもののうちどれだけを予測できたか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 概要\n",
    "- YOLO は他の state-of-the-art な手法に比べ，多様な欠点に悩まされてきた\n",
    "    - localization errors がとても多い\n",
    "    - recall が小さい (FP は少ないけど FN が多い，つまり見落としてる)\n",
    "        - 識別率を維持しつつ，recall と localization を improve する方針\n",
    "- 最近のトレンドはネットワークを深くしたりアンサンブルによりパフォーマンスを上げることだけど，YOLO は速さを維持したいのでそうしなかった\n",
    "\n",
    "### Batch Normalization\n",
    "- batch normalization を全ての convs につけることで 2% improvement in mAP \n",
    "- 過学習を避けつつ dropout をなくすことができた\n",
    "- batch normalization\n",
    "    - 勾配消失・爆発を防ぐ\n",
    "    - 今までは活性化関数の変更，weights の初期値の事前学習，lr を小さくする，dropout などの手法により対処してきたがこれらが不要に\n",
    "    - 共変量シフト (Covariate Shift): 訓練データと予測データの入力の分布に偏りがあること\n",
    "        - 内部の共変量シフト (Internal Covariate Shift): 隠れ層において層と activation 毎に入力分布が変わること\n",
    "    - まあ要するに途中で conv の出力とかを batch 単位で正規化すること\n",
    "    \n",
    "### High Resolution Classifier\n",
    "- pre-training では 224 x 224 で行なっていたが，448 x 448 のフルサイズで行なった，10 epochs\n",
    "- increase almost 4% mAP\n",
    "\n",
    "### Convolutional With Anchor Boxes\n",
    "- YOLO では FC 層で bounding boxes の座標を得ていたが，Faster R-CNN では hand-picked priors と呼ばれる convs のみの layer で得ている\n",
    "    - conv layers しか使っていないので，Faster R-CNN の region proposal network (RPN) はoffsets and confidences for anchor boxes を予測する\n",
    "    - YOLOv2 でもこれを採用\n",
    "- 変更点は以下\n",
    "    1. FC 層をなくして anchor boxes を使った\n",
    "    1. 解像度を上げるために pooling なくした\n",
    "    1. 入力画像を 448 x 448 から 416 x 416 にした\n",
    "        - 32 の奇数倍にして center cell を一意に定めるため\n",
    "        - 13 x 13 の feature map を得る\n",
    "    1. class prediction を spatial location から切り離し，それぞれの anchor box について class と objectness を予測するようにした\n",
    "- anchor box の採用による影響\n",
    "    - accuracy は若干下がった\n",
    "    - YOLO では 98 boxes しか予測できなかったが，千以上の box について予測できるようになった\n",
    "        - without anchor box: 69.5mAP with a recall of 81%\n",
    "        - with anchor box: 69.2mAP with a recall of 88%\n",
    "    - anchor box is 何?\n",
    "        - 単純に各 sliding-window に対して複数の scale, aspect ratio の bounding box をやる\n",
    "\n",
    "### Dimension Clusters\n",
    "- YOLO で anchor boxes を使用することによる2つの問題点のうちの1つ目：box dimensions are hand picked について\n",
    "    - network は box の adjust を学習することができるが，適切な prior (前例，優先順位) を設定することでその学習をより容易にすることができる\n",
    "    - prior を人が決定するのではなく，k-means clustering により行う\n",
    "    - ユークリッド距離によってクラスタリングを行うとでかい box が大きな error を出してしまうので，$d = 1 - \\text{IOU}$ として定義した\n",
    "    - IOU は $k$ と正の相関を持ったが，model complexity と recall とのトレードオフで $k=5$ とした\n",
    "    - $k = 5$ で hand-picked な 9 anchor box と同等の性能 (Ave. IOU = 61%)，$k = 9$ では 67.2%\n",
    "\n",
    "### Direct Location Prediction\n",
    "- YOLO で anchor boxes を使用することによる2つの問題点のうちの2つ目：特に学習初期におけるモデルの不安定性\n",
    "    - 主に $(x,y)$ を予測するところに起因\n",
    "    - Region Proposal Networks では $t_x, t_y$ を導入して解決していたが，これは任意の box を出力できる代わりに学習が大変\n",
    "    - 今回は YOLO を踏襲して grid と bounding box の中心を対応づける\n",
    "    - 以下の $t_x, t_y, t_w, t_h, t_o$ を予測する (grid cell の左上を $(c_x, c_y)$ とする)\n",
    "$$\n",
    "b_x = \\sigma(t_x) + c_x \\\\\n",
    "b_y = \\sigma(t_y) + c_y \\\\\n",
    "b_w = p_we^{t_w}\\\\\n",
    "b_h = p_he^{t_h}\\\\\n",
    "Pr(\\text{object})\\times IOU(b, \\text{object})=\\sigma(t_o)\n",
    "$$\n",
    "    - 学習が容易になったので anchor box に比べ 5% の性能上昇\n",
    "\n",
    "### Fine-Grained Features\n",
    "- 13 x 13 は小さな object には不十分なことがある\n",
    "    - Faster R-CNN, SSD では複数の scale の feature maps に proposal networks をつないでいたが，YOLO では 26 x 26 の feature map からの passthrough を導入することにより解決する\n",
    "- passthrough layer では higher resolution layer を lower resolution layer に concat\n",
    "    - 1つのチャンネルから4つのチャンネルにつなぐ (ResNet に似てる)\n",
    "        - 26 x 26 x 512 -> 13 x 13 x 2048\n",
    "- 1% の改善\n",
    "\n",
    "### Multi-Scale Training\n",
    "- conv layers のみで構成されているのでサイズ不変\n",
    "- $1/32$ にダウンスケールされるので 10 batchs ごとに一辺の長さを $32$ ずつ $\\{320, 352, ... , 608\\}$ と random に変化させた\n",
    "- input resolution を変えることで速さと正確性の trade off ができるお\n",
    "\n",
    "### Further Experiments\n",
    "- PASCAL VOC 2007: high resolution YOLOv2 が最強\n",
    "- PASCAL VOC 2012: SSD 512 が最強"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster\n",
    "- 多くの frameworks は VGG-16 を base feature extractor として使っているが，224 x 224 の画像1枚に対して 30 billion もの浮動小数点演算を要求するので非効率\n",
    "    - YOLOのカスタムモデルでは 224 x 224 の画像に対して 8.5 billion\n",
    "    - ImageNet での性能を比較，VGG-16 の 90.0% に対して 88.0% を記録\n",
    "\n",
    "### Training for Classification\n",
    "- Darknet-19 を 224 x 224 で pre-train したのちに 448 x 448 で fine-tuning\n",
    "- 1000 classes, 160 epochs using SGD\n",
    "- data augumentation\n",
    "\n",
    "### Training for Detection\n",
    "- Darknet-19 の last conv を detection 用の 3 x 3, 1 x 1 の conv に交換し，passthrough leyer を追加\n",
    "- 160 epochs\n",
    "- data augumentation with the same way as SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stronger\n",
    "- classification と detection を jointly に学習するための mechanism を propose\n",
    "    - クラスラベルのみのデータも detection の学習に使える\n",
    "- training 中は classification と detection の datasets を mix して使う\n",
    "    - detection 用の data に対しては architecture 全体の loss function で backropagate\n",
    "    - classification 用の data に対しては loss fuction のうち識別に関わるの部分のみ backropagate\n",
    "        - But how?\n",
    "- 複数の datasets を使うためにはいくつか解決しなければならない問題がある\n",
    "    - detection のクラスは少なく classification のクラスは多い\n",
    "        - COCO では \"dog\" のみでも ImageNet では \"Norfolk terrier\", \"Yorkshire terrier\", and \"Bedlington terrier\" 等 100 種類もある\n",
    "    - 多くの classification model で用いられる softmax は mutual exclusive (互いに排反) であるが，例えば \"dog\" と \"Norfolk terrier\" は排反ではない\n",
    "- この問題を解決するために multi-label model を使用\n",
    "    \n",
    "### Hierarchical Classification\n",
    "- ImageNet のラベルは WordNet からもって来ている\n",
    "    - WordNet 内の単語はグラフ構造をもつ\n",
    "        - \"Norfolk terrier\" < \"terrier\" < \"hunting dog\" < \"dog\" < \"canine\"\n",
    "    - この階層構造を採用する\n",
    "- WordNet 内のグラフ構造は有向グラフであり，木構造ではない\n",
    "    - \"dog\" が \"canine\" でもあり \"domstic animal\" でもあるように言語は複雑だから\n",
    "    - graph 構造の代わりに，hierarchical tree を作成した\n",
    "    - ImageNet 内の単語について，WordNet のグラフを用いて root node への path を特定\n",
    "        - 多くの場合 path は1つのみ\n",
    "    - そのように構築されたグラフに path を追加したり取り除いたりすることで木を最小化した\n",
    "    - このようにして出来上がった木を WordTree と呼ぶ\n",
    "- 各ノードについて条件付き確率 (ex: $Pr(\\text{Norfolk teriier|terrier})$) を予測する\n",
    "$$\n",
    "Pr(\\text{Norfolk teriier})\n",
    "=Pr(\\text{Norfolk teriier|terrier})\n",
    "\\times Pr(\\text{teriier|hunting dog})\n",
    "\\times\\cdots\\times Pr(\\text{mammal|animal})\n",
    "\\times Pr(\\text{animal|physical object})\n",
    "$$\n",
    "- WordTree を構築するために ImageNet の 1000 クラスから 1369 まで中間ノードを追加\n",
    "    - \"Norfolk terrier\" に対しては \"dog\", \"mammal\" も予測するようにした\n",
    "    - クラス増やしたけど性能は落ちなかった\n",
    "    - 学習していない犬を入力すると，\"dog\" の値は高いが下位の単語の値は全て低くなる\n",
    "- detection の際には，detector が生成した bounding box の予測する tree of probability 中を，最も confidence score が高い path を選択するように探索し，score threshold を超えたらそのクラスを出力とする\n",
    "\n",
    "### Joint Classification and Detection\n",
    "- COCO に ImageNet の top 9000 クラスを追加して 9418 クラスの WordTree を作成\n",
    "    - ImageNet の方がデータ数が多いので COCO からは oversampling した，結果 ImageNet:COCO = 4:1\n",
    "- この WordNet で学習した YOLOv2 が YOLO9000\n",
    "    - ただし，anchor box の k-means clustering は $k=3$ に変更\n",
    "- 学習について\n",
    "    - When YOLOv2 sees a detection image\n",
    "        - 普通にバックプロパゲート\n",
    "        - classification についてはより上位の単語まで学習\n",
    "            - \"dog\" でポシャっても \"German Shepherd” versus “Golden Retriever\" については error を与えない\n",
    "    - When YOLOv2 sees a classification image\n",
    "        - classification loss のみバックプロパゲート\n",
    "            - そのクラスに対して最も高い score を返す bounding box を特定し，その predicted tree について loss を計算\n",
    "        - \"We also assume that the predicted box overlaps what would be the ground truth label by at least .3 IOU\" が何言ってるかわからん"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
